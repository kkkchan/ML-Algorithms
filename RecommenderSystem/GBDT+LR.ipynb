{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过GBDT+LR进行推荐，数据集源自kaggle 2014年的一个比赛。\n",
    "\n",
    "数据集介绍：\n",
    "- `train.csv`: Criteo公司7天内的部分流量，每一行对应其提供的广告。数据按照时间进行排序。\n",
    "- `test.csv`: 是训练集之后一天的数据，格式同`train.csv`。\n",
    "\n",
    "字段：\n",
    "- `Label`: 目标。0，1分别代表未点击和点击\n",
    "- `l1-l13`: 一共13列数值特征，大部分都是计数特征\n",
    "- `C1-C26`: 一共26列分类特征，为32位数据表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler,\\\n",
    "    OneHotEncoder, LabelEncoder\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据导入和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../datasets/criteo's-traffic-data\"\n",
    "path = Path(path)\n",
    "path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = path / 'train.csv'\n",
    "test_path = path / 'test.csv'\n",
    "train_path.exists() and test_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1599, 41), (400, 40))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Label'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_train.columns) - set(df_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中`Id`无必要，可进行去除，另外`train`数据带标签而`test`不带标签，可进行填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns='Id', axis=1, inplace=True)\n",
    "df_test.drop(columns='Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Label'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_train, df_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于数据特征中包含两类，既有数值特征又有分类特征，这两类特征应该分开处理，得事先将定义包含分别包含两类特征的集合数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Label', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10',\n",
       "       'I11', 'I12', 'I13', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8',\n",
       "       'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18',\n",
       "       'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_features = df_train.columns[1:14]\n",
    "category_features = df_train.columns[14:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 构建模型\n",
    "\n",
    "使用GBDT+LR进行处理，首先得分别建立两个模型。\n",
    "\n",
    "注意事项：\n",
    "- LR：需要进行特征处理（数值特征需要归一化，类别特征需要one-hot化）\n",
    "- GBDT：需要进行特征处理（类别特征需要one-hot化）\n",
    "- GBDT+LR：LR的输入不仅是GBDT的输出特征，另外**也包括输入原数据**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 将所有数值特征进行归一化（`MinMaxScaler`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "for col in integer_features:\n",
    "    data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 离散数据one-hot化（`OneHotEncoder`和`pd.get_dummies`都可以）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in category_features:\n",
    "    onehot_features = pd.get_dummies(data[col], prefix=col)\n",
    "    data.drop([col], axis=1, inplace=True)\n",
    "    data = pd.concat([data, onehot_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 13105)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 分割数据集\n",
    "\n",
    "分割训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kang/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train = data[data['Label'] != -1]\n",
    "target = train.pop('Label')\n",
    "test = data[data['Label'] == -1]\n",
    "test.drop(['Label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1599, 13104), (400, 13104))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 构建LR模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(x_train, y_train)\n",
    "\n",
    "# predict_proba输出的是0,1类别的概率，由于目标是正样本，所以取索引1\n",
    "train_logloss = log_loss(y_train, model_lr.predict_proba(x_train)[:, 1])\n",
    "val_logloss = log_loss(y_val, model_lr.predict_proba(x_val)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_logloss:  0.12532530444217038\n",
      "val_logloss:  0.4388871134754885\n"
     ]
    }
   ],
   "source": [
    "print('train_logloss: ', train_logloss)\n",
    "print('val_logloss: ', val_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_lr.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测前10个测试样本的正样本预测概率：\n",
      " [0.64830175 0.83173134 0.27290936 0.03009931 0.12878579 0.16314081\n",
      " 0.55666631 0.0678317  0.02816935 0.26834223]\n"
     ]
    }
   ],
   "source": [
    "print('预测前10个测试样本的正样本预测概率：\\n', y_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GBDT建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_train, df_test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 仅需离散化one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in category_features:\n",
    "    onehot_features = pd.get_dummies(data[col], prefix=col)\n",
    "    data.drop(col, axis=1, inplace=True)\n",
    "    data = pd.concat([data, onehot_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 分割数据集\n",
    "\n",
    "分割训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kang/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train = data[data['Label'] != -1]\n",
    "target = train.pop('Label')\n",
    "test = data[data['Label'] == -1]\n",
    "test.drop(['Label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1599, 13092), (400, 13092))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 GBDT建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain's binary_logloss: 0.510716\tval's binary_logloss: 0.50848\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's binary_logloss: 0.50825\tval's binary_logloss: 0.507845\n",
      "[3]\ttrain's binary_logloss: 0.505972\tval's binary_logloss: 0.507147\n",
      "[4]\ttrain's binary_logloss: 0.503759\tval's binary_logloss: 0.506615\n",
      "[5]\ttrain's binary_logloss: 0.50127\tval's binary_logloss: 0.506343\n",
      "[6]\ttrain's binary_logloss: 0.498882\tval's binary_logloss: 0.506083\n",
      "[7]\ttrain's binary_logloss: 0.496621\tval's binary_logloss: 0.505833\n",
      "[8]\ttrain's binary_logloss: 0.494249\tval's binary_logloss: 0.50551\n",
      "[9]\ttrain's binary_logloss: 0.491893\tval's binary_logloss: 0.505215\n",
      "[10]\ttrain's binary_logloss: 0.489866\tval's binary_logloss: 0.504489\n",
      "[11]\ttrain's binary_logloss: 0.487545\tval's binary_logloss: 0.504268\n",
      "[12]\ttrain's binary_logloss: 0.485274\tval's binary_logloss: 0.503703\n",
      "[13]\ttrain's binary_logloss: 0.48304\tval's binary_logloss: 0.503277\n",
      "[14]\ttrain's binary_logloss: 0.480842\tval's binary_logloss: 0.502699\n",
      "[15]\ttrain's binary_logloss: 0.478697\tval's binary_logloss: 0.502474\n",
      "[16]\ttrain's binary_logloss: 0.476613\tval's binary_logloss: 0.502319\n",
      "[17]\ttrain's binary_logloss: 0.474582\tval's binary_logloss: 0.502025\n",
      "[18]\ttrain's binary_logloss: 0.472593\tval's binary_logloss: 0.501813\n",
      "[19]\ttrain's binary_logloss: 0.470978\tval's binary_logloss: 0.50171\n",
      "[20]\ttrain's binary_logloss: 0.468971\tval's binary_logloss: 0.501115\n",
      "[21]\ttrain's binary_logloss: 0.467105\tval's binary_logloss: 0.500884\n",
      "[22]\ttrain's binary_logloss: 0.465161\tval's binary_logloss: 0.500517\n",
      "[23]\ttrain's binary_logloss: 0.46338\tval's binary_logloss: 0.49988\n",
      "[24]\ttrain's binary_logloss: 0.461502\tval's binary_logloss: 0.499698\n",
      "[25]\ttrain's binary_logloss: 0.459617\tval's binary_logloss: 0.499359\n",
      "[26]\ttrain's binary_logloss: 0.457675\tval's binary_logloss: 0.499162\n",
      "[27]\ttrain's binary_logloss: 0.45581\tval's binary_logloss: 0.498815\n",
      "[28]\ttrain's binary_logloss: 0.453978\tval's binary_logloss: 0.498626\n",
      "[29]\ttrain's binary_logloss: 0.452148\tval's binary_logloss: 0.49821\n",
      "[30]\ttrain's binary_logloss: 0.450685\tval's binary_logloss: 0.497873\n",
      "[31]\ttrain's binary_logloss: 0.448975\tval's binary_logloss: 0.497634\n",
      "[32]\ttrain's binary_logloss: 0.447088\tval's binary_logloss: 0.497359\n",
      "[33]\ttrain's binary_logloss: 0.445419\tval's binary_logloss: 0.497024\n",
      "[34]\ttrain's binary_logloss: 0.443789\tval's binary_logloss: 0.496927\n",
      "[35]\ttrain's binary_logloss: 0.442111\tval's binary_logloss: 0.496603\n",
      "[36]\ttrain's binary_logloss: 0.440575\tval's binary_logloss: 0.496247\n",
      "[37]\ttrain's binary_logloss: 0.438983\tval's binary_logloss: 0.496338\n",
      "[38]\ttrain's binary_logloss: 0.437581\tval's binary_logloss: 0.496027\n",
      "[39]\ttrain's binary_logloss: 0.435998\tval's binary_logloss: 0.495907\n",
      "[40]\ttrain's binary_logloss: 0.434315\tval's binary_logloss: 0.495702\n",
      "[41]\ttrain's binary_logloss: 0.432771\tval's binary_logloss: 0.495583\n",
      "[42]\ttrain's binary_logloss: 0.431253\tval's binary_logloss: 0.495443\n",
      "[43]\ttrain's binary_logloss: 0.429661\tval's binary_logloss: 0.495283\n",
      "[44]\ttrain's binary_logloss: 0.428161\tval's binary_logloss: 0.494809\n",
      "[45]\ttrain's binary_logloss: 0.426592\tval's binary_logloss: 0.494576\n",
      "[46]\ttrain's binary_logloss: 0.425\tval's binary_logloss: 0.494649\n",
      "[47]\ttrain's binary_logloss: 0.423448\tval's binary_logloss: 0.494758\n",
      "[48]\ttrain's binary_logloss: 0.422029\tval's binary_logloss: 0.494883\n",
      "[49]\ttrain's binary_logloss: 0.42049\tval's binary_logloss: 0.494607\n",
      "[50]\ttrain's binary_logloss: 0.419063\tval's binary_logloss: 0.494375\n",
      "[51]\ttrain's binary_logloss: 0.417609\tval's binary_logloss: 0.494442\n",
      "[52]\ttrain's binary_logloss: 0.416172\tval's binary_logloss: 0.494322\n",
      "[53]\ttrain's binary_logloss: 0.414682\tval's binary_logloss: 0.494294\n",
      "[54]\ttrain's binary_logloss: 0.413301\tval's binary_logloss: 0.494282\n",
      "[55]\ttrain's binary_logloss: 0.411913\tval's binary_logloss: 0.494048\n",
      "[56]\ttrain's binary_logloss: 0.410415\tval's binary_logloss: 0.493742\n",
      "[57]\ttrain's binary_logloss: 0.408984\tval's binary_logloss: 0.493693\n",
      "[58]\ttrain's binary_logloss: 0.407558\tval's binary_logloss: 0.493695\n",
      "[59]\ttrain's binary_logloss: 0.406169\tval's binary_logloss: 0.493916\n",
      "[60]\ttrain's binary_logloss: 0.404859\tval's binary_logloss: 0.493798\n",
      "[61]\ttrain's binary_logloss: 0.403468\tval's binary_logloss: 0.493631\n",
      "[62]\ttrain's binary_logloss: 0.402028\tval's binary_logloss: 0.49317\n",
      "[63]\ttrain's binary_logloss: 0.400626\tval's binary_logloss: 0.49302\n",
      "[64]\ttrain's binary_logloss: 0.399395\tval's binary_logloss: 0.492891\n",
      "[65]\ttrain's binary_logloss: 0.398017\tval's binary_logloss: 0.492764\n",
      "[66]\ttrain's binary_logloss: 0.396705\tval's binary_logloss: 0.492916\n",
      "[67]\ttrain's binary_logloss: 0.395255\tval's binary_logloss: 0.492714\n",
      "[68]\ttrain's binary_logloss: 0.393869\tval's binary_logloss: 0.492201\n",
      "[69]\ttrain's binary_logloss: 0.392644\tval's binary_logloss: 0.492093\n",
      "[70]\ttrain's binary_logloss: 0.391525\tval's binary_logloss: 0.491927\n",
      "[71]\ttrain's binary_logloss: 0.390208\tval's binary_logloss: 0.492001\n",
      "[72]\ttrain's binary_logloss: 0.388964\tval's binary_logloss: 0.491771\n",
      "[73]\ttrain's binary_logloss: 0.387719\tval's binary_logloss: 0.491508\n",
      "[74]\ttrain's binary_logloss: 0.386279\tval's binary_logloss: 0.491675\n",
      "[75]\ttrain's binary_logloss: 0.384944\tval's binary_logloss: 0.491379\n",
      "[76]\ttrain's binary_logloss: 0.383559\tval's binary_logloss: 0.491629\n",
      "[77]\ttrain's binary_logloss: 0.382278\tval's binary_logloss: 0.491561\n",
      "[78]\ttrain's binary_logloss: 0.381006\tval's binary_logloss: 0.491599\n",
      "[79]\ttrain's binary_logloss: 0.37971\tval's binary_logloss: 0.491954\n",
      "[80]\ttrain's binary_logloss: 0.378559\tval's binary_logloss: 0.491967\n",
      "[81]\ttrain's binary_logloss: 0.37731\tval's binary_logloss: 0.491626\n",
      "[82]\ttrain's binary_logloss: 0.376173\tval's binary_logloss: 0.491401\n",
      "[83]\ttrain's binary_logloss: 0.374868\tval's binary_logloss: 0.491395\n",
      "[84]\ttrain's binary_logloss: 0.373674\tval's binary_logloss: 0.49189\n",
      "[85]\ttrain's binary_logloss: 0.372457\tval's binary_logloss: 0.491973\n",
      "[86]\ttrain's binary_logloss: 0.371344\tval's binary_logloss: 0.491838\n",
      "[87]\ttrain's binary_logloss: 0.370181\tval's binary_logloss: 0.491928\n",
      "[88]\ttrain's binary_logloss: 0.368979\tval's binary_logloss: 0.492149\n",
      "[89]\ttrain's binary_logloss: 0.367826\tval's binary_logloss: 0.491982\n",
      "[90]\ttrain's binary_logloss: 0.366621\tval's binary_logloss: 0.491952\n",
      "[91]\ttrain's binary_logloss: 0.365416\tval's binary_logloss: 0.492231\n",
      "[92]\ttrain's binary_logloss: 0.364178\tval's binary_logloss: 0.492551\n",
      "[93]\ttrain's binary_logloss: 0.362965\tval's binary_logloss: 0.492366\n",
      "[94]\ttrain's binary_logloss: 0.361951\tval's binary_logloss: 0.492245\n",
      "[95]\ttrain's binary_logloss: 0.360816\tval's binary_logloss: 0.492253\n",
      "[96]\ttrain's binary_logloss: 0.359778\tval's binary_logloss: 0.492136\n",
      "[97]\ttrain's binary_logloss: 0.358687\tval's binary_logloss: 0.491914\n",
      "[98]\ttrain's binary_logloss: 0.357403\tval's binary_logloss: 0.491941\n",
      "[99]\ttrain's binary_logloss: 0.356343\tval's binary_logloss: 0.491991\n",
      "[100]\ttrain's binary_logloss: 0.355305\tval's binary_logloss: 0.491826\n",
      "[101]\ttrain's binary_logloss: 0.35425\tval's binary_logloss: 0.491768\n",
      "[102]\ttrain's binary_logloss: 0.353085\tval's binary_logloss: 0.491889\n",
      "[103]\ttrain's binary_logloss: 0.352047\tval's binary_logloss: 0.491747\n",
      "[104]\ttrain's binary_logloss: 0.351165\tval's binary_logloss: 0.491744\n",
      "[105]\ttrain's binary_logloss: 0.349957\tval's binary_logloss: 0.491611\n",
      "[106]\ttrain's binary_logloss: 0.348878\tval's binary_logloss: 0.491808\n",
      "[107]\ttrain's binary_logloss: 0.347834\tval's binary_logloss: 0.491853\n",
      "[108]\ttrain's binary_logloss: 0.346847\tval's binary_logloss: 0.491836\n",
      "[109]\ttrain's binary_logloss: 0.345821\tval's binary_logloss: 0.491958\n",
      "[110]\ttrain's binary_logloss: 0.344801\tval's binary_logloss: 0.492006\n",
      "[111]\ttrain's binary_logloss: 0.343764\tval's binary_logloss: 0.491822\n",
      "[112]\ttrain's binary_logloss: 0.342737\tval's binary_logloss: 0.491755\n",
      "[113]\ttrain's binary_logloss: 0.341831\tval's binary_logloss: 0.491724\n",
      "[114]\ttrain's binary_logloss: 0.340883\tval's binary_logloss: 0.491802\n",
      "[115]\ttrain's binary_logloss: 0.339894\tval's binary_logloss: 0.491738\n",
      "[116]\ttrain's binary_logloss: 0.338874\tval's binary_logloss: 0.491845\n",
      "[117]\ttrain's binary_logloss: 0.337749\tval's binary_logloss: 0.491674\n",
      "[118]\ttrain's binary_logloss: 0.336667\tval's binary_logloss: 0.491795\n",
      "[119]\ttrain's binary_logloss: 0.335641\tval's binary_logloss: 0.491881\n",
      "[120]\ttrain's binary_logloss: 0.334571\tval's binary_logloss: 0.49222\n",
      "[121]\ttrain's binary_logloss: 0.333595\tval's binary_logloss: 0.492234\n",
      "[122]\ttrain's binary_logloss: 0.332552\tval's binary_logloss: 0.492122\n",
      "[123]\ttrain's binary_logloss: 0.331718\tval's binary_logloss: 0.491987\n",
      "[124]\ttrain's binary_logloss: 0.331017\tval's binary_logloss: 0.491925\n",
      "[125]\ttrain's binary_logloss: 0.33006\tval's binary_logloss: 0.491992\n",
      "[126]\ttrain's binary_logloss: 0.329162\tval's binary_logloss: 0.492231\n",
      "[127]\ttrain's binary_logloss: 0.328002\tval's binary_logloss: 0.49225\n",
      "[128]\ttrain's binary_logloss: 0.326981\tval's binary_logloss: 0.492329\n",
      "[129]\ttrain's binary_logloss: 0.325946\tval's binary_logloss: 0.492272\n",
      "[130]\ttrain's binary_logloss: 0.32513\tval's binary_logloss: 0.492307\n",
      "[131]\ttrain's binary_logloss: 0.32411\tval's binary_logloss: 0.492041\n",
      "[132]\ttrain's binary_logloss: 0.323153\tval's binary_logloss: 0.492244\n",
      "[133]\ttrain's binary_logloss: 0.322233\tval's binary_logloss: 0.492275\n",
      "[134]\ttrain's binary_logloss: 0.321353\tval's binary_logloss: 0.492152\n",
      "[135]\ttrain's binary_logloss: 0.320299\tval's binary_logloss: 0.49232\n",
      "[136]\ttrain's binary_logloss: 0.319459\tval's binary_logloss: 0.492155\n",
      "[137]\ttrain's binary_logloss: 0.318508\tval's binary_logloss: 0.492418\n",
      "[138]\ttrain's binary_logloss: 0.317654\tval's binary_logloss: 0.492431\n",
      "[139]\ttrain's binary_logloss: 0.316813\tval's binary_logloss: 0.492509\n",
      "[140]\ttrain's binary_logloss: 0.316008\tval's binary_logloss: 0.492677\n",
      "[141]\ttrain's binary_logloss: 0.315083\tval's binary_logloss: 0.492618\n",
      "[142]\ttrain's binary_logloss: 0.314101\tval's binary_logloss: 0.492613\n",
      "[143]\ttrain's binary_logloss: 0.313188\tval's binary_logloss: 0.493016\n",
      "[144]\ttrain's binary_logloss: 0.312219\tval's binary_logloss: 0.493165\n",
      "[145]\ttrain's binary_logloss: 0.31123\tval's binary_logloss: 0.493432\n",
      "[146]\ttrain's binary_logloss: 0.31029\tval's binary_logloss: 0.49346\n",
      "[147]\ttrain's binary_logloss: 0.30938\tval's binary_logloss: 0.493581\n",
      "[148]\ttrain's binary_logloss: 0.308532\tval's binary_logloss: 0.493851\n",
      "[149]\ttrain's binary_logloss: 0.307762\tval's binary_logloss: 0.494038\n",
      "[150]\ttrain's binary_logloss: 0.306905\tval's binary_logloss: 0.493898\n",
      "[151]\ttrain's binary_logloss: 0.306031\tval's binary_logloss: 0.494042\n",
      "[152]\ttrain's binary_logloss: 0.305199\tval's binary_logloss: 0.494336\n",
      "[153]\ttrain's binary_logloss: 0.304387\tval's binary_logloss: 0.494307\n",
      "[154]\ttrain's binary_logloss: 0.303514\tval's binary_logloss: 0.494297\n",
      "[155]\ttrain's binary_logloss: 0.302594\tval's binary_logloss: 0.494345\n",
      "[156]\ttrain's binary_logloss: 0.301786\tval's binary_logloss: 0.494588\n",
      "[157]\ttrain's binary_logloss: 0.300865\tval's binary_logloss: 0.494494\n",
      "[158]\ttrain's binary_logloss: 0.299974\tval's binary_logloss: 0.494681\n",
      "[159]\ttrain's binary_logloss: 0.299148\tval's binary_logloss: 0.494709\n",
      "[160]\ttrain's binary_logloss: 0.298179\tval's binary_logloss: 0.494916\n",
      "[161]\ttrain's binary_logloss: 0.297395\tval's binary_logloss: 0.494926\n",
      "[162]\ttrain's binary_logloss: 0.296531\tval's binary_logloss: 0.495046\n",
      "[163]\ttrain's binary_logloss: 0.295716\tval's binary_logloss: 0.495212\n",
      "[164]\ttrain's binary_logloss: 0.2948\tval's binary_logloss: 0.495376\n",
      "[165]\ttrain's binary_logloss: 0.294025\tval's binary_logloss: 0.495251\n",
      "[166]\ttrain's binary_logloss: 0.29325\tval's binary_logloss: 0.495493\n",
      "[167]\ttrain's binary_logloss: 0.292434\tval's binary_logloss: 0.495667\n",
      "[168]\ttrain's binary_logloss: 0.291927\tval's binary_logloss: 0.49554\n",
      "[169]\ttrain's binary_logloss: 0.291201\tval's binary_logloss: 0.495482\n",
      "[170]\ttrain's binary_logloss: 0.290627\tval's binary_logloss: 0.495195\n",
      "[171]\ttrain's binary_logloss: 0.289704\tval's binary_logloss: 0.495472\n",
      "[172]\ttrain's binary_logloss: 0.288943\tval's binary_logloss: 0.495156\n",
      "[173]\ttrain's binary_logloss: 0.288169\tval's binary_logloss: 0.495028\n",
      "[174]\ttrain's binary_logloss: 0.287467\tval's binary_logloss: 0.495073\n",
      "[175]\ttrain's binary_logloss: 0.286787\tval's binary_logloss: 0.495132\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttrain's binary_logloss: 0.384944\tval's binary_logloss: 0.491379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.7, learning_rate=0.01, max_depth=12,\n",
       "               min_child_weight=0.5, n_estimators=10000, num_leaves=100,\n",
       "               objective='binary', subsample=0.8)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt = lightgbm.LGBMClassifier(boosting_type='gbdt', \n",
    "                              objective='binary', \n",
    "                              subsample=0.8,\n",
    "                              min_child_weight=0.5, \n",
    "                              colsample_bytree=0.7,\n",
    "                              num_leaves=100,\n",
    "                              max_depth=12,\n",
    "                              learning_rate=0.01,\n",
    "                              n_estimators=10000\n",
    "                              )\n",
    "gbdt.fit(x_train, y_train, eval_set=[(x_train, y_train),\n",
    "                                    (x_val, y_val)],\n",
    "         eval_names=['train', 'val'],\n",
    "         eval_metric='binary_logloss',\n",
    "         early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logloss = log_loss(y_train, gbdt.predict_proba(x_train)[:, 1])   # −(ylog(p)+(1−y)log(1−p)) log_loss\n",
    "val_logloss = log_loss(y_val, gbdt.predict_proba(x_val)[:, 1])\n",
    "y_pred = gbdt.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_logloss:  0.38494440600725727\n",
      "val_logloss:  0.491378914243493\n",
      "预测前10个测试样本的正样本预测概率：\n",
      " [0.32814256 0.27857479 0.18389449 0.1756473  0.17750179 0.32951145\n",
      " 0.18588754 0.15332876 0.14050926 0.20831841]\n"
     ]
    }
   ],
   "source": [
    "print('train_logloss: ', train_logloss)\n",
    "print('val_logloss: ', val_logloss)\n",
    "print('预测前10个测试样本的正样本预测概率：\\n', y_pred[:10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 GBDT+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_train, df_test], axis=0)\n",
    "data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kang/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "for col in category_features:\n",
    "    onehot_features = pd.get_dummies(data[col], prefix=col)\n",
    "    data.drop(col, axis=1, inplace=True)\n",
    "    data = pd.concat([data, onehot_features], axis=1)\n",
    "\n",
    "train = data[data['Label'] != -1]\n",
    "target = train.pop('Label')\n",
    "test = data[data['Label'] == -1]\n",
    "test.drop(['Label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train, target, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain's binary_logloss: 0.514356\tval's binary_logloss: 0.49594\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's binary_logloss: 0.51211\tval's binary_logloss: 0.495314\n",
      "[3]\ttrain's binary_logloss: 0.509881\tval's binary_logloss: 0.494604\n",
      "[4]\ttrain's binary_logloss: 0.507836\tval's binary_logloss: 0.493983\n",
      "[5]\ttrain's binary_logloss: 0.505763\tval's binary_logloss: 0.493119\n",
      "[6]\ttrain's binary_logloss: 0.503774\tval's binary_logloss: 0.492661\n",
      "[7]\ttrain's binary_logloss: 0.501896\tval's binary_logloss: 0.491738\n",
      "[8]\ttrain's binary_logloss: 0.499895\tval's binary_logloss: 0.49112\n",
      "[9]\ttrain's binary_logloss: 0.497814\tval's binary_logloss: 0.490488\n",
      "[10]\ttrain's binary_logloss: 0.49592\tval's binary_logloss: 0.489721\n",
      "[11]\ttrain's binary_logloss: 0.493976\tval's binary_logloss: 0.489026\n",
      "[12]\ttrain's binary_logloss: 0.491928\tval's binary_logloss: 0.488457\n",
      "[13]\ttrain's binary_logloss: 0.490179\tval's binary_logloss: 0.487714\n",
      "[14]\ttrain's binary_logloss: 0.48831\tval's binary_logloss: 0.486601\n",
      "[15]\ttrain's binary_logloss: 0.486476\tval's binary_logloss: 0.486107\n",
      "[16]\ttrain's binary_logloss: 0.484654\tval's binary_logloss: 0.485446\n",
      "[17]\ttrain's binary_logloss: 0.482815\tval's binary_logloss: 0.484752\n",
      "[18]\ttrain's binary_logloss: 0.481035\tval's binary_logloss: 0.484156\n",
      "[19]\ttrain's binary_logloss: 0.479269\tval's binary_logloss: 0.483591\n",
      "[20]\ttrain's binary_logloss: 0.477453\tval's binary_logloss: 0.483123\n",
      "[21]\ttrain's binary_logloss: 0.475742\tval's binary_logloss: 0.482608\n",
      "[22]\ttrain's binary_logloss: 0.473996\tval's binary_logloss: 0.482115\n",
      "[23]\ttrain's binary_logloss: 0.472254\tval's binary_logloss: 0.481194\n",
      "[24]\ttrain's binary_logloss: 0.470558\tval's binary_logloss: 0.480378\n",
      "[25]\ttrain's binary_logloss: 0.468891\tval's binary_logloss: 0.479849\n",
      "[26]\ttrain's binary_logloss: 0.467156\tval's binary_logloss: 0.479188\n",
      "[27]\ttrain's binary_logloss: 0.465512\tval's binary_logloss: 0.478786\n",
      "[28]\ttrain's binary_logloss: 0.464067\tval's binary_logloss: 0.478227\n",
      "[29]\ttrain's binary_logloss: 0.462452\tval's binary_logloss: 0.477288\n",
      "[30]\ttrain's binary_logloss: 0.460854\tval's binary_logloss: 0.476508\n",
      "[31]\ttrain's binary_logloss: 0.45926\tval's binary_logloss: 0.476333\n",
      "[32]\ttrain's binary_logloss: 0.457763\tval's binary_logloss: 0.475661\n",
      "[33]\ttrain's binary_logloss: 0.456224\tval's binary_logloss: 0.475354\n",
      "[34]\ttrain's binary_logloss: 0.45485\tval's binary_logloss: 0.474939\n",
      "[35]\ttrain's binary_logloss: 0.453185\tval's binary_logloss: 0.474291\n",
      "[36]\ttrain's binary_logloss: 0.451634\tval's binary_logloss: 0.473502\n",
      "[37]\ttrain's binary_logloss: 0.450195\tval's binary_logloss: 0.473371\n",
      "[38]\ttrain's binary_logloss: 0.448622\tval's binary_logloss: 0.473031\n",
      "[39]\ttrain's binary_logloss: 0.446989\tval's binary_logloss: 0.472979\n",
      "[40]\ttrain's binary_logloss: 0.445434\tval's binary_logloss: 0.472255\n",
      "[41]\ttrain's binary_logloss: 0.443913\tval's binary_logloss: 0.471727\n",
      "[42]\ttrain's binary_logloss: 0.442307\tval's binary_logloss: 0.471212\n",
      "[43]\ttrain's binary_logloss: 0.440934\tval's binary_logloss: 0.470847\n",
      "[44]\ttrain's binary_logloss: 0.439524\tval's binary_logloss: 0.470391\n",
      "[45]\ttrain's binary_logloss: 0.437936\tval's binary_logloss: 0.470057\n",
      "[46]\ttrain's binary_logloss: 0.436636\tval's binary_logloss: 0.469638\n",
      "[47]\ttrain's binary_logloss: 0.435052\tval's binary_logloss: 0.469112\n",
      "[48]\ttrain's binary_logloss: 0.433706\tval's binary_logloss: 0.468893\n",
      "[49]\ttrain's binary_logloss: 0.432278\tval's binary_logloss: 0.468479\n",
      "[50]\ttrain's binary_logloss: 0.431091\tval's binary_logloss: 0.468214\n",
      "[51]\ttrain's binary_logloss: 0.430002\tval's binary_logloss: 0.467648\n",
      "[52]\ttrain's binary_logloss: 0.428511\tval's binary_logloss: 0.467443\n",
      "[53]\ttrain's binary_logloss: 0.427192\tval's binary_logloss: 0.467244\n",
      "[54]\ttrain's binary_logloss: 0.425918\tval's binary_logloss: 0.467051\n",
      "[55]\ttrain's binary_logloss: 0.424591\tval's binary_logloss: 0.466839\n",
      "[56]\ttrain's binary_logloss: 0.423064\tval's binary_logloss: 0.466356\n",
      "[57]\ttrain's binary_logloss: 0.421628\tval's binary_logloss: 0.466075\n",
      "[58]\ttrain's binary_logloss: 0.420327\tval's binary_logloss: 0.465648\n",
      "[59]\ttrain's binary_logloss: 0.419093\tval's binary_logloss: 0.465421\n",
      "[60]\ttrain's binary_logloss: 0.417724\tval's binary_logloss: 0.465413\n",
      "[61]\ttrain's binary_logloss: 0.416513\tval's binary_logloss: 0.464931\n",
      "[62]\ttrain's binary_logloss: 0.415091\tval's binary_logloss: 0.464153\n",
      "[63]\ttrain's binary_logloss: 0.41383\tval's binary_logloss: 0.463691\n",
      "[64]\ttrain's binary_logloss: 0.412486\tval's binary_logloss: 0.463352\n",
      "[65]\ttrain's binary_logloss: 0.411339\tval's binary_logloss: 0.463326\n",
      "[66]\ttrain's binary_logloss: 0.410041\tval's binary_logloss: 0.462913\n",
      "[67]\ttrain's binary_logloss: 0.408751\tval's binary_logloss: 0.462616\n",
      "[68]\ttrain's binary_logloss: 0.407447\tval's binary_logloss: 0.462252\n",
      "[69]\ttrain's binary_logloss: 0.406383\tval's binary_logloss: 0.462198\n",
      "[70]\ttrain's binary_logloss: 0.40505\tval's binary_logloss: 0.461767\n",
      "[71]\ttrain's binary_logloss: 0.403812\tval's binary_logloss: 0.461663\n",
      "[72]\ttrain's binary_logloss: 0.402664\tval's binary_logloss: 0.461543\n",
      "[73]\ttrain's binary_logloss: 0.401346\tval's binary_logloss: 0.461416\n",
      "[74]\ttrain's binary_logloss: 0.400227\tval's binary_logloss: 0.46123\n",
      "[75]\ttrain's binary_logloss: 0.398997\tval's binary_logloss: 0.460805\n",
      "[76]\ttrain's binary_logloss: 0.397853\tval's binary_logloss: 0.460517\n",
      "[77]\ttrain's binary_logloss: 0.396661\tval's binary_logloss: 0.460243\n",
      "[78]\ttrain's binary_logloss: 0.395426\tval's binary_logloss: 0.460369\n",
      "[79]\ttrain's binary_logloss: 0.394311\tval's binary_logloss: 0.460202\n",
      "[80]\ttrain's binary_logloss: 0.393142\tval's binary_logloss: 0.460022\n",
      "[81]\ttrain's binary_logloss: 0.392044\tval's binary_logloss: 0.459997\n",
      "[82]\ttrain's binary_logloss: 0.390782\tval's binary_logloss: 0.459798\n",
      "[83]\ttrain's binary_logloss: 0.389701\tval's binary_logloss: 0.459605\n",
      "[84]\ttrain's binary_logloss: 0.388672\tval's binary_logloss: 0.459608\n",
      "[85]\ttrain's binary_logloss: 0.387524\tval's binary_logloss: 0.459303\n",
      "[86]\ttrain's binary_logloss: 0.386504\tval's binary_logloss: 0.459116\n",
      "[87]\ttrain's binary_logloss: 0.385366\tval's binary_logloss: 0.458849\n",
      "[88]\ttrain's binary_logloss: 0.384334\tval's binary_logloss: 0.458885\n",
      "[89]\ttrain's binary_logloss: 0.383203\tval's binary_logloss: 0.458783\n",
      "[90]\ttrain's binary_logloss: 0.382172\tval's binary_logloss: 0.458516\n",
      "[91]\ttrain's binary_logloss: 0.380955\tval's binary_logloss: 0.458358\n",
      "[92]\ttrain's binary_logloss: 0.379913\tval's binary_logloss: 0.458162\n",
      "[93]\ttrain's binary_logloss: 0.378808\tval's binary_logloss: 0.457921\n",
      "[94]\ttrain's binary_logloss: 0.377777\tval's binary_logloss: 0.457746\n",
      "[95]\ttrain's binary_logloss: 0.376762\tval's binary_logloss: 0.457952\n",
      "[96]\ttrain's binary_logloss: 0.375703\tval's binary_logloss: 0.457845\n",
      "[97]\ttrain's binary_logloss: 0.374596\tval's binary_logloss: 0.457589\n",
      "[98]\ttrain's binary_logloss: 0.373426\tval's binary_logloss: 0.457308\n",
      "[99]\ttrain's binary_logloss: 0.372374\tval's binary_logloss: 0.457172\n",
      "[100]\ttrain's binary_logloss: 0.371224\tval's binary_logloss: 0.456877\n",
      "[101]\ttrain's binary_logloss: 0.370232\tval's binary_logloss: 0.456429\n",
      "[102]\ttrain's binary_logloss: 0.369123\tval's binary_logloss: 0.456464\n",
      "[103]\ttrain's binary_logloss: 0.368095\tval's binary_logloss: 0.456202\n",
      "[104]\ttrain's binary_logloss: 0.366978\tval's binary_logloss: 0.455672\n",
      "[105]\ttrain's binary_logloss: 0.366034\tval's binary_logloss: 0.4555\n",
      "[106]\ttrain's binary_logloss: 0.365008\tval's binary_logloss: 0.455452\n",
      "[107]\ttrain's binary_logloss: 0.364195\tval's binary_logloss: 0.454965\n",
      "[108]\ttrain's binary_logloss: 0.363242\tval's binary_logloss: 0.454851\n",
      "[109]\ttrain's binary_logloss: 0.362192\tval's binary_logloss: 0.454158\n",
      "[110]\ttrain's binary_logloss: 0.361127\tval's binary_logloss: 0.454118\n",
      "[111]\ttrain's binary_logloss: 0.360128\tval's binary_logloss: 0.453743\n",
      "[112]\ttrain's binary_logloss: 0.359426\tval's binary_logloss: 0.453574\n",
      "[113]\ttrain's binary_logloss: 0.358446\tval's binary_logloss: 0.453275\n",
      "[114]\ttrain's binary_logloss: 0.357446\tval's binary_logloss: 0.453136\n",
      "[115]\ttrain's binary_logloss: 0.356586\tval's binary_logloss: 0.452998\n",
      "[116]\ttrain's binary_logloss: 0.355656\tval's binary_logloss: 0.453049\n",
      "[117]\ttrain's binary_logloss: 0.354783\tval's binary_logloss: 0.453156\n",
      "[118]\ttrain's binary_logloss: 0.353957\tval's binary_logloss: 0.453024\n",
      "[119]\ttrain's binary_logloss: 0.352978\tval's binary_logloss: 0.452712\n",
      "[120]\ttrain's binary_logloss: 0.351982\tval's binary_logloss: 0.452515\n",
      "[121]\ttrain's binary_logloss: 0.351277\tval's binary_logloss: 0.452426\n",
      "[122]\ttrain's binary_logloss: 0.350421\tval's binary_logloss: 0.452126\n",
      "[123]\ttrain's binary_logloss: 0.349531\tval's binary_logloss: 0.451753\n",
      "[124]\ttrain's binary_logloss: 0.348727\tval's binary_logloss: 0.451321\n",
      "[125]\ttrain's binary_logloss: 0.347884\tval's binary_logloss: 0.450965\n",
      "[126]\ttrain's binary_logloss: 0.347109\tval's binary_logloss: 0.450856\n",
      "[127]\ttrain's binary_logloss: 0.346144\tval's binary_logloss: 0.450877\n",
      "[128]\ttrain's binary_logloss: 0.345326\tval's binary_logloss: 0.450969\n",
      "[129]\ttrain's binary_logloss: 0.344357\tval's binary_logloss: 0.450808\n",
      "[130]\ttrain's binary_logloss: 0.343406\tval's binary_logloss: 0.450788\n",
      "[131]\ttrain's binary_logloss: 0.342404\tval's binary_logloss: 0.450813\n",
      "[132]\ttrain's binary_logloss: 0.341605\tval's binary_logloss: 0.4509\n",
      "[133]\ttrain's binary_logloss: 0.340666\tval's binary_logloss: 0.450674\n",
      "[134]\ttrain's binary_logloss: 0.339703\tval's binary_logloss: 0.450526\n",
      "[135]\ttrain's binary_logloss: 0.338767\tval's binary_logloss: 0.450245\n",
      "[136]\ttrain's binary_logloss: 0.338001\tval's binary_logloss: 0.450303\n",
      "[137]\ttrain's binary_logloss: 0.337185\tval's binary_logloss: 0.449989\n",
      "[138]\ttrain's binary_logloss: 0.336449\tval's binary_logloss: 0.449969\n",
      "[139]\ttrain's binary_logloss: 0.335683\tval's binary_logloss: 0.449829\n",
      "[140]\ttrain's binary_logloss: 0.334947\tval's binary_logloss: 0.44999\n",
      "[141]\ttrain's binary_logloss: 0.334018\tval's binary_logloss: 0.449858\n",
      "[142]\ttrain's binary_logloss: 0.333291\tval's binary_logloss: 0.44965\n",
      "[143]\ttrain's binary_logloss: 0.332317\tval's binary_logloss: 0.449641\n",
      "[144]\ttrain's binary_logloss: 0.331326\tval's binary_logloss: 0.449698\n",
      "[145]\ttrain's binary_logloss: 0.330449\tval's binary_logloss: 0.449768\n",
      "[146]\ttrain's binary_logloss: 0.329747\tval's binary_logloss: 0.449672\n",
      "[147]\ttrain's binary_logloss: 0.328962\tval's binary_logloss: 0.449382\n",
      "[148]\ttrain's binary_logloss: 0.328538\tval's binary_logloss: 0.449349\n",
      "[149]\ttrain's binary_logloss: 0.327603\tval's binary_logloss: 0.449592\n",
      "[150]\ttrain's binary_logloss: 0.326739\tval's binary_logloss: 0.449744\n",
      "[151]\ttrain's binary_logloss: 0.325876\tval's binary_logloss: 0.449832\n",
      "[152]\ttrain's binary_logloss: 0.324975\tval's binary_logloss: 0.449777\n",
      "[153]\ttrain's binary_logloss: 0.324184\tval's binary_logloss: 0.449969\n",
      "[154]\ttrain's binary_logloss: 0.323337\tval's binary_logloss: 0.449847\n",
      "[155]\ttrain's binary_logloss: 0.322513\tval's binary_logloss: 0.449984\n",
      "[156]\ttrain's binary_logloss: 0.321616\tval's binary_logloss: 0.4499\n",
      "[157]\ttrain's binary_logloss: 0.320867\tval's binary_logloss: 0.449927\n",
      "[158]\ttrain's binary_logloss: 0.320118\tval's binary_logloss: 0.449724\n",
      "[159]\ttrain's binary_logloss: 0.319345\tval's binary_logloss: 0.449728\n",
      "[160]\ttrain's binary_logloss: 0.318544\tval's binary_logloss: 0.449561\n",
      "[161]\ttrain's binary_logloss: 0.317761\tval's binary_logloss: 0.449808\n",
      "[162]\ttrain's binary_logloss: 0.317048\tval's binary_logloss: 0.449781\n",
      "[163]\ttrain's binary_logloss: 0.31626\tval's binary_logloss: 0.449881\n",
      "[164]\ttrain's binary_logloss: 0.315286\tval's binary_logloss: 0.449752\n",
      "[165]\ttrain's binary_logloss: 0.314391\tval's binary_logloss: 0.449715\n",
      "[166]\ttrain's binary_logloss: 0.313627\tval's binary_logloss: 0.449419\n",
      "[167]\ttrain's binary_logloss: 0.312953\tval's binary_logloss: 0.449218\n",
      "[168]\ttrain's binary_logloss: 0.312176\tval's binary_logloss: 0.449095\n",
      "[169]\ttrain's binary_logloss: 0.311301\tval's binary_logloss: 0.44898\n",
      "[170]\ttrain's binary_logloss: 0.310462\tval's binary_logloss: 0.448865\n",
      "[171]\ttrain's binary_logloss: 0.309569\tval's binary_logloss: 0.448976\n",
      "[172]\ttrain's binary_logloss: 0.308846\tval's binary_logloss: 0.449014\n",
      "[173]\ttrain's binary_logloss: 0.308125\tval's binary_logloss: 0.44892\n",
      "[174]\ttrain's binary_logloss: 0.307358\tval's binary_logloss: 0.448972\n",
      "[175]\ttrain's binary_logloss: 0.306642\tval's binary_logloss: 0.448979\n",
      "[176]\ttrain's binary_logloss: 0.305785\tval's binary_logloss: 0.449059\n",
      "[177]\ttrain's binary_logloss: 0.305131\tval's binary_logloss: 0.449129\n",
      "[178]\ttrain's binary_logloss: 0.304287\tval's binary_logloss: 0.449104\n",
      "[179]\ttrain's binary_logloss: 0.303599\tval's binary_logloss: 0.449071\n",
      "[180]\ttrain's binary_logloss: 0.302898\tval's binary_logloss: 0.448959\n",
      "[181]\ttrain's binary_logloss: 0.302119\tval's binary_logloss: 0.4489\n",
      "[182]\ttrain's binary_logloss: 0.301468\tval's binary_logloss: 0.448949\n",
      "[183]\ttrain's binary_logloss: 0.300788\tval's binary_logloss: 0.448894\n",
      "[184]\ttrain's binary_logloss: 0.30021\tval's binary_logloss: 0.448682\n",
      "[185]\ttrain's binary_logloss: 0.299458\tval's binary_logloss: 0.44872\n",
      "[186]\ttrain's binary_logloss: 0.298854\tval's binary_logloss: 0.448754\n",
      "[187]\ttrain's binary_logloss: 0.298036\tval's binary_logloss: 0.448677\n",
      "[188]\ttrain's binary_logloss: 0.297369\tval's binary_logloss: 0.448772\n",
      "[189]\ttrain's binary_logloss: 0.296626\tval's binary_logloss: 0.448725\n",
      "[190]\ttrain's binary_logloss: 0.296101\tval's binary_logloss: 0.448552\n",
      "[191]\ttrain's binary_logloss: 0.295631\tval's binary_logloss: 0.448346\n",
      "[192]\ttrain's binary_logloss: 0.294774\tval's binary_logloss: 0.448558\n",
      "[193]\ttrain's binary_logloss: 0.294186\tval's binary_logloss: 0.448569\n",
      "[194]\ttrain's binary_logloss: 0.293443\tval's binary_logloss: 0.448444\n",
      "[195]\ttrain's binary_logloss: 0.292874\tval's binary_logloss: 0.448298\n",
      "[196]\ttrain's binary_logloss: 0.292317\tval's binary_logloss: 0.448255\n",
      "[197]\ttrain's binary_logloss: 0.291695\tval's binary_logloss: 0.447981\n",
      "[198]\ttrain's binary_logloss: 0.291046\tval's binary_logloss: 0.447926\n",
      "[199]\ttrain's binary_logloss: 0.290453\tval's binary_logloss: 0.447968\n",
      "[200]\ttrain's binary_logloss: 0.289736\tval's binary_logloss: 0.448089\n",
      "[201]\ttrain's binary_logloss: 0.289208\tval's binary_logloss: 0.447979\n",
      "[202]\ttrain's binary_logloss: 0.288498\tval's binary_logloss: 0.447794\n",
      "[203]\ttrain's binary_logloss: 0.287764\tval's binary_logloss: 0.447902\n",
      "[204]\ttrain's binary_logloss: 0.287138\tval's binary_logloss: 0.448049\n",
      "[205]\ttrain's binary_logloss: 0.286757\tval's binary_logloss: 0.447897\n",
      "[206]\ttrain's binary_logloss: 0.286055\tval's binary_logloss: 0.448016\n",
      "[207]\ttrain's binary_logloss: 0.285385\tval's binary_logloss: 0.448009\n",
      "[208]\ttrain's binary_logloss: 0.284945\tval's binary_logloss: 0.447958\n",
      "[209]\ttrain's binary_logloss: 0.284241\tval's binary_logloss: 0.447838\n",
      "[210]\ttrain's binary_logloss: 0.283634\tval's binary_logloss: 0.447606\n",
      "[211]\ttrain's binary_logloss: 0.283007\tval's binary_logloss: 0.447416\n",
      "[212]\ttrain's binary_logloss: 0.282582\tval's binary_logloss: 0.447347\n",
      "[213]\ttrain's binary_logloss: 0.282038\tval's binary_logloss: 0.447015\n",
      "[214]\ttrain's binary_logloss: 0.281618\tval's binary_logloss: 0.446942\n",
      "[215]\ttrain's binary_logloss: 0.281071\tval's binary_logloss: 0.44695\n",
      "[216]\ttrain's binary_logloss: 0.280602\tval's binary_logloss: 0.446771\n",
      "[217]\ttrain's binary_logloss: 0.280056\tval's binary_logloss: 0.446655\n",
      "[218]\ttrain's binary_logloss: 0.279455\tval's binary_logloss: 0.446902\n",
      "[219]\ttrain's binary_logloss: 0.278777\tval's binary_logloss: 0.446852\n",
      "[220]\ttrain's binary_logloss: 0.27805\tval's binary_logloss: 0.447034\n",
      "[221]\ttrain's binary_logloss: 0.27744\tval's binary_logloss: 0.447111\n",
      "[222]\ttrain's binary_logloss: 0.276936\tval's binary_logloss: 0.447091\n",
      "[223]\ttrain's binary_logloss: 0.276376\tval's binary_logloss: 0.447131\n",
      "[224]\ttrain's binary_logloss: 0.275778\tval's binary_logloss: 0.447395\n",
      "[225]\ttrain's binary_logloss: 0.275275\tval's binary_logloss: 0.447446\n",
      "[226]\ttrain's binary_logloss: 0.274879\tval's binary_logloss: 0.447342\n",
      "[227]\ttrain's binary_logloss: 0.274163\tval's binary_logloss: 0.447515\n",
      "[228]\ttrain's binary_logloss: 0.273612\tval's binary_logloss: 0.447845\n",
      "[229]\ttrain's binary_logloss: 0.273122\tval's binary_logloss: 0.447746\n",
      "[230]\ttrain's binary_logloss: 0.272767\tval's binary_logloss: 0.447693\n",
      "[231]\ttrain's binary_logloss: 0.272226\tval's binary_logloss: 0.447424\n",
      "[232]\ttrain's binary_logloss: 0.271618\tval's binary_logloss: 0.447266\n",
      "[233]\ttrain's binary_logloss: 0.271125\tval's binary_logloss: 0.447282\n",
      "[234]\ttrain's binary_logloss: 0.270616\tval's binary_logloss: 0.447192\n",
      "[235]\ttrain's binary_logloss: 0.270014\tval's binary_logloss: 0.44698\n",
      "[236]\ttrain's binary_logloss: 0.269278\tval's binary_logloss: 0.447006\n",
      "[237]\ttrain's binary_logloss: 0.268613\tval's binary_logloss: 0.447087\n",
      "[238]\ttrain's binary_logloss: 0.268076\tval's binary_logloss: 0.447001\n",
      "[239]\ttrain's binary_logloss: 0.267722\tval's binary_logloss: 0.447017\n",
      "[240]\ttrain's binary_logloss: 0.26732\tval's binary_logloss: 0.44705\n",
      "[241]\ttrain's binary_logloss: 0.266672\tval's binary_logloss: 0.447243\n",
      "[242]\ttrain's binary_logloss: 0.266292\tval's binary_logloss: 0.447336\n",
      "[243]\ttrain's binary_logloss: 0.265543\tval's binary_logloss: 0.447494\n",
      "[244]\ttrain's binary_logloss: 0.26505\tval's binary_logloss: 0.447535\n",
      "[245]\ttrain's binary_logloss: 0.264403\tval's binary_logloss: 0.447614\n",
      "[246]\ttrain's binary_logloss: 0.263898\tval's binary_logloss: 0.447659\n",
      "[247]\ttrain's binary_logloss: 0.263366\tval's binary_logloss: 0.447908\n",
      "[248]\ttrain's binary_logloss: 0.262712\tval's binary_logloss: 0.448108\n",
      "[249]\ttrain's binary_logloss: 0.262206\tval's binary_logloss: 0.448114\n",
      "[250]\ttrain's binary_logloss: 0.261685\tval's binary_logloss: 0.448083\n",
      "[251]\ttrain's binary_logloss: 0.261193\tval's binary_logloss: 0.44793\n",
      "[252]\ttrain's binary_logloss: 0.26072\tval's binary_logloss: 0.447936\n",
      "[253]\ttrain's binary_logloss: 0.260215\tval's binary_logloss: 0.448178\n",
      "[254]\ttrain's binary_logloss: 0.259553\tval's binary_logloss: 0.44821\n",
      "[255]\ttrain's binary_logloss: 0.25889\tval's binary_logloss: 0.448492\n",
      "[256]\ttrain's binary_logloss: 0.258521\tval's binary_logloss: 0.448292\n",
      "[257]\ttrain's binary_logloss: 0.258028\tval's binary_logloss: 0.44829\n",
      "[258]\ttrain's binary_logloss: 0.257435\tval's binary_logloss: 0.448297\n",
      "[259]\ttrain's binary_logloss: 0.256876\tval's binary_logloss: 0.448374\n",
      "[260]\ttrain's binary_logloss: 0.256507\tval's binary_logloss: 0.448353\n",
      "[261]\ttrain's binary_logloss: 0.256115\tval's binary_logloss: 0.448399\n",
      "[262]\ttrain's binary_logloss: 0.255588\tval's binary_logloss: 0.448316\n",
      "[263]\ttrain's binary_logloss: 0.25512\tval's binary_logloss: 0.448289\n",
      "[264]\ttrain's binary_logloss: 0.254881\tval's binary_logloss: 0.448163\n",
      "[265]\ttrain's binary_logloss: 0.254465\tval's binary_logloss: 0.44841\n",
      "[266]\ttrain's binary_logloss: 0.254101\tval's binary_logloss: 0.448407\n",
      "[267]\ttrain's binary_logloss: 0.25365\tval's binary_logloss: 0.44829\n",
      "[268]\ttrain's binary_logloss: 0.253181\tval's binary_logloss: 0.448367\n",
      "[269]\ttrain's binary_logloss: 0.252735\tval's binary_logloss: 0.448637\n",
      "[270]\ttrain's binary_logloss: 0.252412\tval's binary_logloss: 0.448728\n",
      "[271]\ttrain's binary_logloss: 0.252096\tval's binary_logloss: 0.448713\n",
      "[272]\ttrain's binary_logloss: 0.251675\tval's binary_logloss: 0.448551\n",
      "[273]\ttrain's binary_logloss: 0.251144\tval's binary_logloss: 0.448649\n",
      "[274]\ttrain's binary_logloss: 0.250718\tval's binary_logloss: 0.448537\n",
      "[275]\ttrain's binary_logloss: 0.25023\tval's binary_logloss: 0.448618\n",
      "[276]\ttrain's binary_logloss: 0.24975\tval's binary_logloss: 0.448535\n",
      "[277]\ttrain's binary_logloss: 0.249299\tval's binary_logloss: 0.448547\n",
      "[278]\ttrain's binary_logloss: 0.248877\tval's binary_logloss: 0.44844\n",
      "[279]\ttrain's binary_logloss: 0.248472\tval's binary_logloss: 0.448317\n",
      "[280]\ttrain's binary_logloss: 0.248101\tval's binary_logloss: 0.448258\n",
      "[281]\ttrain's binary_logloss: 0.247749\tval's binary_logloss: 0.448348\n",
      "[282]\ttrain's binary_logloss: 0.247491\tval's binary_logloss: 0.448324\n",
      "[283]\ttrain's binary_logloss: 0.246984\tval's binary_logloss: 0.448298\n",
      "[284]\ttrain's binary_logloss: 0.246803\tval's binary_logloss: 0.448175\n",
      "[285]\ttrain's binary_logloss: 0.246246\tval's binary_logloss: 0.44835\n",
      "[286]\ttrain's binary_logloss: 0.246058\tval's binary_logloss: 0.448231\n",
      "[287]\ttrain's binary_logloss: 0.245538\tval's binary_logloss: 0.448247\n",
      "[288]\ttrain's binary_logloss: 0.245271\tval's binary_logloss: 0.448263\n",
      "[289]\ttrain's binary_logloss: 0.244693\tval's binary_logloss: 0.448146\n",
      "[290]\ttrain's binary_logloss: 0.244292\tval's binary_logloss: 0.448029\n",
      "[291]\ttrain's binary_logloss: 0.243781\tval's binary_logloss: 0.448184\n",
      "[292]\ttrain's binary_logloss: 0.243374\tval's binary_logloss: 0.448145\n",
      "[293]\ttrain's binary_logloss: 0.242866\tval's binary_logloss: 0.4484\n",
      "[294]\ttrain's binary_logloss: 0.242587\tval's binary_logloss: 0.448655\n",
      "[295]\ttrain's binary_logloss: 0.242104\tval's binary_logloss: 0.448733\n",
      "[296]\ttrain's binary_logloss: 0.241683\tval's binary_logloss: 0.448799\n",
      "[297]\ttrain's binary_logloss: 0.241388\tval's binary_logloss: 0.44873\n",
      "[298]\ttrain's binary_logloss: 0.241156\tval's binary_logloss: 0.448682\n",
      "[299]\ttrain's binary_logloss: 0.240857\tval's binary_logloss: 0.44879\n",
      "[300]\ttrain's binary_logloss: 0.240563\tval's binary_logloss: 0.448925\n",
      "[301]\ttrain's binary_logloss: 0.240105\tval's binary_logloss: 0.449075\n",
      "[302]\ttrain's binary_logloss: 0.239606\tval's binary_logloss: 0.449001\n",
      "[303]\ttrain's binary_logloss: 0.239387\tval's binary_logloss: 0.448842\n",
      "[304]\ttrain's binary_logloss: 0.238869\tval's binary_logloss: 0.449089\n",
      "[305]\ttrain's binary_logloss: 0.23836\tval's binary_logloss: 0.44913\n",
      "[306]\ttrain's binary_logloss: 0.237874\tval's binary_logloss: 0.449168\n",
      "[307]\ttrain's binary_logloss: 0.237422\tval's binary_logloss: 0.44932\n",
      "[308]\ttrain's binary_logloss: 0.236905\tval's binary_logloss: 0.449277\n",
      "[309]\ttrain's binary_logloss: 0.236591\tval's binary_logloss: 0.449253\n",
      "[310]\ttrain's binary_logloss: 0.236137\tval's binary_logloss: 0.449408\n",
      "[311]\ttrain's binary_logloss: 0.235765\tval's binary_logloss: 0.449275\n",
      "[312]\ttrain's binary_logloss: 0.235389\tval's binary_logloss: 0.449274\n",
      "[313]\ttrain's binary_logloss: 0.234941\tval's binary_logloss: 0.449306\n",
      "[314]\ttrain's binary_logloss: 0.234346\tval's binary_logloss: 0.449287\n",
      "[315]\ttrain's binary_logloss: 0.233974\tval's binary_logloss: 0.449335\n",
      "[316]\ttrain's binary_logloss: 0.233526\tval's binary_logloss: 0.449558\n",
      "[317]\ttrain's binary_logloss: 0.233232\tval's binary_logloss: 0.449474\n",
      "Early stopping, best iteration is:\n",
      "[217]\ttrain's binary_logloss: 0.280056\tval's binary_logloss: 0.446655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.7, learning_rate=0.01, max_depth=12,\n",
       "               min_child_weight=0.5, n_estimators=1000, num_leaves=100,\n",
       "               objective='binary', subsample=0.8)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt = lightgbm.LGBMClassifier(boosting_type='gbdt',\n",
    "                        objective='binary',\n",
    "                        subsample= 0.8,\n",
    "                        min_child_weight= 0.5,\n",
    "                        colsample_bytree= 0.7,\n",
    "                        num_leaves=100,\n",
    "                        max_depth = 12,\n",
    "                        learning_rate=0.01,\n",
    "                        n_estimators=1000,\n",
    "                        )\n",
    "gbdt.fit(x_train, y_train, \n",
    "        eval_set=[(x_train, y_train), (x_val, y_val)], \n",
    "        eval_names=['train', 'val'],\n",
    "        eval_metric='binary_logloss',\n",
    "        early_stopping_rounds=100,\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gbdt.booster_\n",
    "\n",
    "gbdt_feats_train = model.predict(train, pred_leaf=True)\n",
    "gbdt_feats_test = model.predict(test, pred_leaf = True)\n",
    "gbdt_feats_name = ['gbdt_leaf_' + str(i) for i in range(gbdt_feats_train.shape[1])]\n",
    "df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns = gbdt_feats_name) \n",
    "df_test_gbdt_feats = pd.DataFrame(gbdt_feats_test, columns = gbdt_feats_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, df_train_gbdt_feats], axis = 1)\n",
    "test = pd.concat([test, df_test_gbdt_feats], axis = 1)\n",
    "train_len = train.shape[0]\n",
    "data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "for col in integer_features:\n",
    "    data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "\n",
    "for col in gbdt_feats_name:\n",
    "    onehot_feats = pd.get_dummies(data[col], prefix = col)\n",
    "    data.drop([col], axis = 1, inplace = True)\n",
    "    data = pd.concat([data, onehot_feats], axis = 1)\n",
    "\n",
    "train = data[: train_len]\n",
    "test = data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train, target, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr-logloss:  0.009539733478411328\n",
      "val-logloss:  0.23676042150340368\n",
      "预测前10个测试样本的正样本预测概率：\n",
      " [8.45006675e-01 1.71889779e-02 3.96333726e-01 3.70922173e-03\n",
      " 1.06322840e-02 1.98127020e-01 4.46584975e-03 6.12527110e-03\n",
      " 8.19511513e-05 3.08945681e-01]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "tr_logloss = log_loss(y_train, lr.predict_proba(x_train)[:, 1])\n",
    "print('tr-logloss: ', tr_logloss)\n",
    "val_logloss = log_loss(y_val, lr.predict_proba(x_val)[:, 1])\n",
    "print('val-logloss: ', val_logloss)\n",
    "y_pred = lr.predict_proba(test)[:, 1]\n",
    "print('预测前10个测试样本的正样本预测概率：\\n', y_pred[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比可见，GBDT+LR这套方案相交各个单独使用大大减少了loss值，无论是在训练集还是测试集。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
