{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Tensoflow2实现GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据导入\n",
    "\n",
    "依旧使用Cora数据集，跟GCN的实现中一样，所有这里仅仅只是把GCN实现中的代码搬了过来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_path = '../datasets/cora/cora.content'\n",
    "cite_path = '../datasets/cora/cora.cites'\n",
    "data_dir = '../datasets/cora'\n",
    "os.path.exists(content_path), os.path.exists(cite_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(content_path, 'r') as f:\n",
    "    contents = f.readlines()\n",
    "with open(cite_path, 'r') as f:\n",
    "    cites = f.readlines()\n",
    "contents = np.array([l.strip().split('\\t') for l in contents])\n",
    "cites_raw = np.array([i.strip().split('\\t') for i in cites])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============图数据信息=============\n",
      "节点数量： 2708\n",
      "边数量： 5429\n",
      "特征维数： 1433\n",
      "标签类别数量： 7\n",
      "标签类别：\n",
      "     - Case_Based\n",
      "     - Genetic_Algorithms\n",
      "     - Neural_Networks\n",
      "     - Probabilistic_Methods\n",
      "     - Reinforcement_Learning\n",
      "     - Rule_Learning\n",
      "     - Theory\n"
     ]
    }
   ],
   "source": [
    "papers_raw, features_raw, labels_raw = np.split(contents, [1, -1], axis=1)\n",
    "features = features_raw.astype(np.float32)\n",
    "\n",
    "paper_dict = {key:value for value, key in enumerate(np.squeeze(papers_raw))}\n",
    "label_dict = {key:value for value, key in enumerate(np.unique(np.squeeze(labels_raw)))}\n",
    "\n",
    "papers = np.array([[paper_dict[key]] for key in papers_raw.reshape(-1)])\n",
    "labels = np.array([[label_dict[key]] for key in labels_raw.reshape(-1)])\n",
    "\n",
    "cites = np.array([[paper_dict[i[0]], paper_dict[i[1]]] for i in cites_raw])\n",
    "node_num = len(papers)\n",
    "label_num = len(label_dict.keys())\n",
    "feature_dim = features.shape[1]\n",
    "edge_num = len(cites)\n",
    "\n",
    "print('{:=^30}'.format('图数据信息'))\n",
    "print('节点数量：', node_num)\n",
    "print('边数量：', edge_num)\n",
    "print('特征维数：', feature_dim)\n",
    "print('标签类别数量：', label_num)\n",
    "print('标签类别：')\n",
    "for label in label_dict.keys():\n",
    "    print('{: <5}- {:<}'.format('', label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Case_Based': 0,\n",
       " 'Genetic_Algorithms': 1,\n",
       " 'Neural_Networks': 2,\n",
       " 'Probabilistic_Methods': 3,\n",
       " 'Reinforcement_Learning': 4,\n",
       " 'Rule_Learning': 5,\n",
       " 'Theory': 6}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为方便后续使用，将处理好的数据进行离线保存\n",
    "data = {'papers': papers, 'labels': labels, 'cites': cites, 'features': features, \n",
    "       'paper_dict': paper_dict, 'label_dict': label_dict, 'node_num': node_num, \n",
    "       'edge_num': edge_num, 'label_num': label_num, 'feature_dim': feature_dim}\n",
    "\n",
    "import pickle \n",
    "pickle.dump(data, open(os.path.join(data_dir, 'cora_data.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2708, 1), (2708, 1), (5429, 2), (2708, 1433))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.shape, labels.shape, cites.shape, features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 5429, 7, 1433)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_num, edge_num, label_num, feature_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 构造邻接矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(papers)))\n",
    "G.add_edges_from(cites)\n",
    "adj_matrix = nx.adjacency_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2708, 2708), scipy.sparse.csr.csr_matrix)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix.shape, type(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将稀疏矩阵转为普通矩阵，并引入自环边，构造带自环的邻接矩阵\n",
    "A = adj_matrix.toarray()\n",
    "A = A + np.eye(A.shape[0])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点数：2708\n"
     ]
    }
   ],
   "source": [
    "print(f'节点数：{node_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 数据集分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集分割方式：\n",
    "- 训练集：**每个类别中取出20个**作为训练集（共140）\n",
    "- 验证集：剩下数据集中取出**500个**\n",
    "- 测试集：剩下数据集中取出**1000个**\n",
    "\n",
    "由于计算时是需要所有的图数据和特征，所以数据集分割采用的是mask数组，**只在算损失和准确率时使用**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 4, ..., 1, 0, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_ = labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.zeros(node_num, dtype=np.uint8)\n",
    "val_mask = np.zeros(node_num, dtype=np.uint8)\n",
    "test_mask = np.zeros(node_num, dtype=np.uint8)\n",
    "pivot1, pivot2 = int(0.6 * node_num), int(0.8 * node_num)\n",
    "train_mask[:pivot1] = 1\n",
    "val_mask[pivot1:pivot2] = 1\n",
    "test_mask[pivot2:] = 1\n",
    "train_mask = tf.cast(train_mask, tf.bool)\n",
    "val_mask = tf.cast(val_mask, tf.bool)\n",
    "test_mask = tf.cast(test_mask, tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ = np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  11,   22,   38,   42,   53,   55,   65,  100,  120,  129,  139,\n",
       "        141,  145,  163,  168,  188,  189,  191,  209,  219,  228,  237,\n",
       "        240,  243,  265,  273,  290,  309,  329,  343,  357,  361,  372,\n",
       "        380,  390,  395,  402,  403,  415,  422,  448,  475,  493,  523,\n",
       "        530,  537,  538,  546,  602,  606,  658,  681,  689,  712,  714,\n",
       "        717,  721,  727,  728,  743,  744,  755,  757,  758,  764,  765,\n",
       "        769,  780,  781,  789,  793,  800,  803,  811,  813,  815,  833,\n",
       "        844,  846,  854,  856,  860,  880,  900,  902,  910,  934,  935,\n",
       "        938,  940,  941,  942,  943,  953,  956,  958,  959,  961,  964,\n",
       "        965,  966,  970,  972,  974,  982,  995, 1009, 1016, 1018, 1030,\n",
       "       1034, 1035, 1036, 1054, 1060, 1063, 1065, 1069, 1070, 1077, 1096,\n",
       "       1098, 1099, 1106, 1110, 1113, 1114, 1117, 1127, 1130, 1131, 1134,\n",
       "       1136, 1139, 1140, 1142, 1143, 1153, 1159, 1167, 1175, 1178, 1184,\n",
       "       1204, 1205, 1206, 1207, 1216, 1218, 1219, 1221, 1224, 1225, 1230,\n",
       "       1231, 1232, 1249, 1257, 1261, 1264, 1274, 1277, 1279, 1286, 1299,\n",
       "       1303, 1305, 1311, 1319, 1325, 1330, 1331, 1332, 1333, 1334, 1351,\n",
       "       1353, 1362, 1369, 1371, 1379, 1380, 1391, 1392, 1395, 1396, 1404,\n",
       "       1408, 1409, 1410, 1424, 1432, 1439, 1441, 1455, 1466, 1467, 1473,\n",
       "       1474, 1476, 1481, 1495, 1516, 1518, 1523, 1530, 1533, 1536, 1541,\n",
       "       1544, 1558, 1563, 1565, 1571, 1572, 1575, 1577, 1580, 1588, 1590,\n",
       "       1594, 1598, 1603, 1611, 1628, 1631, 1638, 1646, 1650, 1662, 1663,\n",
       "       1670, 1673, 1675, 1685, 1688, 1689, 1690, 1691, 1694, 1696, 1715,\n",
       "       1717, 1719, 1728, 1729, 1730, 1734, 1736, 1737, 1738, 1740, 1743,\n",
       "       1745, 1756, 1769, 1771, 1775, 1784, 1785, 1790, 1796, 1797, 1799,\n",
       "       1807, 1826, 1831, 1832, 1834, 1836, 1839, 1840, 1850, 1872, 1878,\n",
       "       1880, 1883, 1890, 1905, 1911, 1917, 1921, 1925, 1930, 1931, 1936,\n",
       "       1940, 1943, 1946, 1950, 1958, 1959, 1965, 1971, 1973, 1980, 1984,\n",
       "       1985, 1999, 2001, 2003, 2030, 2039, 2047, 2055, 2058, 2065, 2077,\n",
       "       2082, 2086, 2087, 2089, 2101, 2102, 2103, 2104, 2111, 2116, 2130,\n",
       "       2133, 2139, 2152, 2165, 2170, 2173, 2175, 2177, 2192, 2193, 2196,\n",
       "       2199, 2200, 2202, 2204, 2205, 2206, 2211, 2216, 2220, 2226, 2232,\n",
       "       2237, 2238, 2248, 2249, 2250, 2251, 2252, 2254, 2255, 2257, 2259,\n",
       "       2261, 2264, 2265, 2267, 2269, 2271, 2274, 2277, 2280, 2286, 2295,\n",
       "       2296, 2298, 2302, 2316, 2317, 2330, 2332, 2334, 2347, 2353, 2361,\n",
       "       2363, 2379, 2392, 2396, 2402, 2407, 2429, 2446, 2451, 2470, 2478,\n",
       "       2479, 2490, 2504, 2512, 2515, 2516, 2518, 2519, 2521, 2541, 2554,\n",
       "       2563, 2564, 2598, 2600, 2604, 2613, 2624, 2630, 2637, 2638, 2641,\n",
       "       2643, 2646, 2659, 2664, 2667, 2673, 2688, 2702, 2703, 2704, 2705])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(labels_))[labels_==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT实验设置：\n",
    "\n",
    "- 参数初始化：Glorot\n",
    "- 优化算法：Adam\n",
    "- 学习率：0.005\n",
    "- 提早停止：True\n",
    "- epochs：100\n",
    "- 层数：2\n",
    "- 第一层：8heads，$F\\prime = 8$，64个输出特征，ELU作激活函数\n",
    "- 第二层：1heads，$F\\prime = classes$，$classes$个输出特征，softmax激活函数\n",
    "- 正则化：L2，$\\lambda = 0.0005$\n",
    "- Dropout：0.6，用在每层输入和**规范化注意力系数**\n",
    "\n",
    "GCN实验设置：\n",
    "\n",
    "计算公式：$$Z = softmax(\\hat A ReLU(\\hat A X W^{(0)})W^{(1)}))$$\n",
    "\n",
    "- Dropout：0.5，所有层\n",
    "- 正则化：L2，$\\lambda = 0.0005$，第一层\n",
    "- 隐层数：1\n",
    "- 隐层神经元数：16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_conv(keras.layers.Layer):\n",
    "    def __init__(self, units, A_hat, node_num, activation, dropout_rate=0.0, activation=None,\n",
    "                 use_bias=Ture, kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros', coef_dropout=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.use_bias = bias\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units, ),\n",
    "                                initializer='random_normal', trainable=True)\n",
    "        \n",
    "    def call(self, inputs, A_hat):\n",
    "        return self.activation(A_hat @ inputs @ self.W + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_sparse_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"预测结果是概率形式，标签是正确类型，计算\"\"\"\n",
    "    loss = - tf.math.log(tf.clip_by_value(preds[mask], 1e-7, 1)) * tf.one_hot(labels[mask].ravel(), 7)\n",
    "    loss = tf.reduce_sum(loss, axis=1)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    '''\n",
    "    或者\n",
    "    keras.losses.SparseCategoricalCrossentropy(from_logits=False)(labels[mask], preds[mask])\n",
    "    '''\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct = tf.equal(tf.argmax(preds[mask], axis=1), labels[mask].ravel())\n",
    "    correct = tf.cast(correct, tf.float32)\n",
    "    return tf.reduce_mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
