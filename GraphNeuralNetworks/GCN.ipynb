{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Pytorch实现GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据\n",
    "\n",
    "采用Cora数据集。该数据集是一个论文图，共2708个节点，每个节点都是一篇论文，所有样本点被分为7类别：\n",
    "\n",
    "1. Case_Based\n",
    "2. Genetic_Algorithms\n",
    "3. Neural_Networks\n",
    "4. Probabilistic_Methods\n",
    "5. Reinforcement_Learning\n",
    "6. Rule_Learning\n",
    "7. Theory\n",
    "\n",
    "每篇论文都由一个1433维的词向量表示，即节点特征维度为1433。词向量的每个特征都对应一个词，取0表示该特征对应的词不在论文中，取1则表示在论文中。每篇论文都至少引用了一篇其他论文，或者被其他论文引用，这是一个连通图，不存在孤立点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = '../datasets/cora/cora.content'\n",
    "cite_path = '../datasets/cora/cora.cites'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(content_path), os.path.exists(cite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始数据分别存于变量`contents`, `cites`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(content_path, 'r') as f:\n",
    "    contents = f.readlines()\n",
    "with open(cite_path, 'r') as f:\n",
    "    cites = f.readlines()\n",
    "contents = np.array([l.strip().split('\\t') for l in contents])\n",
    "cites_raw = np.array([i.strip().split('\\t') for i in cites])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['31336', '0', '0', ..., '0', '0', 'Neural_Networks'],\n",
       "        ['1061127', '0', '0', ..., '0', '0', 'Rule_Learning'],\n",
       "        ['1106406', '0', '0', ..., '0', '0', 'Reinforcement_Learning'],\n",
       "        ...,\n",
       "        ['1128978', '0', '0', ..., '0', '0', 'Genetic_Algorithms'],\n",
       "        ['117328', '0', '0', ..., '0', '0', 'Case_Based'],\n",
       "        ['24043', '0', '0', ..., '0', '0', 'Neural_Networks']],\n",
       "       dtype='<U22'),\n",
       " array([['35', '1033'],\n",
       "        ['35', '103482'],\n",
       "        ['35', '103515'],\n",
       "        ...,\n",
       "        ['853118', '1140289'],\n",
       "        ['853155', '853118'],\n",
       "        ['954315', '1155073']], dtype='<U7'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents, cites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理\n",
    "\n",
    "对原始数据进行预处理，提取有效信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据，第二个参数是分割点，也可以使用np.hsplit，就不用指定axis\n",
    "papers_raw, features, labels_raw = np.split(contents, [1, -1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2708, 1), (2708, 1433), (2708, 1))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_raw.shape, features.shape, labels_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一个**映射字典**，值为自然序数，键为论文代号\n",
    "\n",
    "同样建立一个字典，值为自然序数，键为论文类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_dict = {key:value for value, key in enumerate(np.squeeze(papers_raw))}\n",
    "label_dict = {key:value for value, key in enumerate(np.unique(np.squeeze(labels_raw)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将论文和论文类别通过字典进行映射，同时将引用中的论文也通过字典进行映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = np.array([[paper_dict[key]] for key in papers_raw.reshape(-1)])\n",
    "labels = np.array([[label_dict[key]] for key in labels_raw.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "cites = np.array([[paper_dict[i[0]], paper_dict[i[1]]] for i in cites_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引用数据为单向的，为了构建**双向边**，将引用数据反转一次边后连接起来，此时边数变为了原来的两倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cites = np.concatenate((cites, cites[:, ::-1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = len(papers)\n",
    "label_num = len(label_dict.keys())\n",
    "feature_dim = features.shape[1]\n",
    "edge_num = len(cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============数据信息=============\n",
      "节点数量： 2708\n",
      "边数量： 5429\n",
      "特征维数： 1433\n",
      "标签种类数量： 7\n",
      "标签种类：\n",
      "     - Case_Based\n",
      "     - Genetic_Algorithms\n",
      "     - Neural_Networks\n",
      "     - Probabilistic_Methods\n",
      "     - Reinforcement_Learning\n",
      "     - Rule_Learning\n",
      "     - Theory\n"
     ]
    }
   ],
   "source": [
    "print('{:=^30}'.format('数据信息'))\n",
    "print('节点数量：', node_num)\n",
    "print('边数量：', edge_num)\n",
    "print('特征维数：', feature_dim)\n",
    "print('标签种类数量：', label_num)\n",
    "print('标签种类：')\n",
    "for label in label_dict.keys():\n",
    "    print('{: <5}- {:<}'.format('', label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过networx构建无向图，并构建**邻接矩阵**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(papers)))\n",
    "G.add_edges_from(cites)\n",
    "adj_matrix = nx.adjacency_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 2708)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, ..., 4, 4, 3], dtype=int64)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(adj_matrix.sum(axis=0)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "邻接矩阵**正规化函数**，返回值对应论文中的：\n",
    "$$\n",
    "\\hat A = D^{-1/2}(A+I)D^{-1/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"\n",
    "    Convert sparse matrix to tuple representation.\n",
    "    \"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def normalize_adjacent(adj):\n",
    "    adj += sp.eye(adj.shape[0])\n",
    "    degree = np.array(adj.sum(axis=1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    norm_adj = d_hat @ adj @ d_hat\n",
    "    return norm_adj\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    adj_normalized = normalize_adjacent(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat = normalize_adjacent(adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据分割\n",
    "\n",
    "按8：2进行数据分割，分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.zeros(node_num, dtype=np.uint8)\n",
    "val_mask = np.zeros(node_num, dtype=np.uint8)\n",
    "test_mask = np.zeros(node_num, dtype=np.uint8)\n",
    "pivot = int(node_num * 0.6)\n",
    "train_mask[:pivot] = 1\n",
    "pivot1 = int(node_num * 0.8)\n",
    "val_mask[pivot: pivot1] = 1 \n",
    "test_mask[pivot1: ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2166), tensor(542))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask.sum(), test_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'values' and 'dense_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-fb8a9a70a601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# features = tf.SparseTensor(sparse_to_tuple(features))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mA_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'values' and 'dense_shape'"
     ]
    }
   ],
   "source": [
    "train_mask = tf.convert_to_tensor(train_mask)\n",
    "val_mask = tf.convert_to_tensor(val_mask)\n",
    "test_mask = tf.convert_to_tensor(test_mask)\n",
    "# features = tf.SparseTensor(sparse_to_tuple(features))\n",
    "A_hat = tf.SparseTensor(sparse_to_tuple(A_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算：\n",
    "状态转移方程\n",
    "$$\n",
    "H^n = \\sigma(\\hat A H^{n-1} W^{n-1})\n",
    "$$\n",
    "其中$\\hat A$为正规化邻接矩阵，计算如下：\n",
    "$$\n",
    "\\hat A = D^{-1/2}(A+I)D^{-1/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       "  PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(), tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=False, \n",
    "                 activation=None, with_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.A_hat = placeholders['A_hat']\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0\n",
    "        self.with_bias = with_bias\n",
    "    \n",
    "    def build(self):\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel', shape=[self.input_dim, self.output_dim], \n",
    "            initializer=self.kernel_initializer)\n",
    "        if self.with_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                name='bias', shape=[self.output_dim], initializer='zeros')\n",
    "    \n",
    "    def call(self, X):\n",
    "        output = self.A_hat @ X @ self.kernel\n",
    "        if self.with_bias:\n",
    "            output += self.bias\n",
    "        return self.activation(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(keras.Model):\n",
    "    def __init__(self, placeholders, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = placeholders['input_dim']\n",
    "        self.hidden_dim = placeholders['hidden_dim']\n",
    "        self.output_dim = placeholders['output_dim']\n",
    "        self.placeholders = placeholders\n",
    "        \n",
    "        self.slayers = []\n",
    "        self.slayers.append(GCNConv(self.input_dim, self.hidden_dim, placeholders,\n",
    "                                   activation=tf.nn.relu))\n",
    "        self.slayers.append(GCNConv(self.hidden_dim, self.output_dim, placeholders,\n",
    "                                  activation=lambda x:x))\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        x, label, mask = inputs\n",
    "        output = x\n",
    "        for layer in self.slayers:\n",
    "            output = layer(output)\n",
    "#         loss = masked_softmax_cross_entropy(output[-1], self.placeholders['labels'],\n",
    "#                                            self.placeholders['labels_mask'])\n",
    "#         self.add_loss(loss)\n",
    "        loss += masked_softmax_cross_entropy(output, label, mask)\n",
    "\n",
    "        acc = masked_accuracy(output, label, mask)\n",
    "        return loss, acc\n",
    "    \n",
    "#     def predict(self):\n",
    "#         return tf.nn.softmax(self.outputs)\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         shape = tf.TensorShape(input_shape).as_list()\n",
    "#         shape[-1] = self.num_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholders = {\n",
    "    'A_hat': A_hat,\n",
    "    'features': features,\n",
    "    'input_dim': feature_dim,\n",
    "    'hidden_dim': 32,\n",
    "    'output_dim': label_num,\n",
    "}\n",
    "model = GCN(placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-293-87688bc30985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-2f8f519db216>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#         loss = masked_softmax_cross_entropy(output[-1], self.placeholders['labels'],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#                                            self.placeholders['labels_mask'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: build() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=1e-2)\n",
    "for epoch in range(200):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, acc = model(features)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    _, val_acc = model(features, training=False)\n",
    "\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "\n",
    "        print(epoch, float(loss), float(acc), '\\tval:', float(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features, labels, validation_data=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNmodel(keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input = keras.layers.Input(shape=(input_dim,))\n",
    "        self.conv1 = GCNConv(hidden_dim, activation='relu')\n",
    "        self.conv2 = GCNConv(output_dim, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = self.conv1(inputs)\n",
    "        Z = self.conv2(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型训练和评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
